# chinese_ngram_entorpy_jinyong
如何对信息进行量化度量是计算机对于信息进行高效处理的前提，一直以来研究者都在不断尝试寻找最为合理解决的方法，这引起了许多人的思考。衡量信息量即为衡量信息价值，信息带来的效益。我们可以认为，信息量的度量就等于不确定性的多少。本文参考Peter Brown文章来计算中文的平均信息熵。选取数据集为16本金庸武侠小说，搭建三种n-gram模型，计算得出不同分词下的中文信息熵。本文通过对比三种n-gram语言模型（1-gram、2-gram、3-gram）得到的结果，分析得出n的取值越大，即在估计时考虑的词数越多，则上下文之间的联系越多，不同词组合出现的种类个数也会越多，则文本的信息熵则越小的结论。
同时，文章在此基础上研究了繁体字对于信息熵的影响。在使用繁体字替换之后，词库总词数和不同词的个数都有所减少。这可能是由于jieba分词模块对于简体字的识别率更高，从而减少了个别字单独划分的现象。同时，可以看到除了2-gram以外，替换繁体字的信息熵都有所减少。
